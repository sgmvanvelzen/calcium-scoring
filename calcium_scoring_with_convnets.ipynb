{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic calcium scoring in chest CT using CNNs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A frequently performed task in various types of images is calcium scoring in computed tomography (CT) exams for cardiovascular disease (CVD) risk prediction. Calcium scoring is routinely performed in non-contrast enhanced ECG-triggered cardiac CT, but increasingly applied in other CT exams visualizing the heart.\n",
    "\n",
    "This document describes a method based on deep convolutional neural networks for automatic detection and segmentation of arterial calcifications in CT scans. This method is developed for low-dose chest CT and has been published in TMI:\n",
    "\n",
    "> N. Lessmann, B. van Ginneken, M. Zreik, P.A. de Jong, B.D. de Vos, M.A. Viergever, I. Isgum. Automatic calcium scoring in low-dose chest CT using deep neural networks with dilated convolutions. IEEE Transactions on Medical Imaging 2018, volume 37(2), pp. 615-625, [DOI 10.1109/TMI.2017.2769839](https://dx.doi.org/10.1109/TMI.2017.2769839)\n",
    "\n",
    "Moreover, an extensive evaluation of the method for calcium scoring in multiple Cardiac CT and Chest CT protocols has been performed. This work is submitted to Radiology: \n",
    "\n",
    "> S.G.M. van Velzen, N. Lessmann, B.K. Velthuis, I.E.M. Bank, D.H.J.G. van den Bongard, T. Leiner, P.A. de Jong, W.B. Veldhuis, A. Correa, J.G. Terry, J.J. Carr, M.A. Viergever, H.M. Verkooijen, I. Išgum. Deep learning for automatic calcium scoring in CT: Validation using multiple Cardiac CT and Chest CT protocols. Submitted to Radiology. \n",
    "\n",
    "\n",
    "## Python packages\n",
    "\n",
    "Fundamental components on which this code is build are:\n",
    "\n",
    "* **Python 3.6**\n",
    "* **Numpy**\n",
    "* **Scipy**\n",
    "* **Theano** (0.9) and **Lasagne** (0.2dev)\n",
    "* **SimpleITK**\n",
    "\n",
    "Additionally, **visdom** can used to monitor the training progress.\n",
    "\n",
    "#### This notebook is a step-by-step guide for training and testing the calcium scoring method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subdirectory to include path\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "print(sys.version)\n",
    "\n",
    "# Include custom library\n",
    "import calciumscoring\n",
    "\n",
    "# Include other libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# Shortcuts\n",
    "path = os.path\n",
    "\n",
    "# Define input and scratch directory locations\n",
    "if path.exists('/home/user'):\n",
    "    input_dir = '/home/user/input'\n",
    "    scratch_dir = '/home/user/scratch'\n",
    "else:\n",
    "    input_dir = path.join(os.getcwd(), 'input')\n",
    "    scratch_dir = path.join(os.getcwd(), 'scratch')\n",
    "    \n",
    "    if not path.exists(scratch_dir):\n",
    "        os.makedirs(scratch_dir)\n",
    "\n",
    "\n",
    "# Use visdom to display learning curves while the model is training?\n",
    "# Set to empty string (\"\") to disable\n",
    "visdom_server = 'http://your_server:8082'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging data\n",
    "\n",
    "In the paper by Lessmann et al., data from the National Lung Screening Trial (NLST) was used for development and evaluation. The NLST was a large lung cancer screening trial in the United States that compared chest X-ray with low-dose chest CT for early detection of lung cancer.\n",
    "\n",
    "For calcium scoring, a subset was selected from the 6000 available baseline scans, i.e., the scans in the first screening round. We used a diverse subset of in total 1744 scans, of which eventually 1687 scans were useable (this was determined by the human observers who manually annotated calcifications in these scans, see \"Manual annotation\" section below). For further details regarding the subset selection, please refer to the TMI paper cited above.\n",
    "\n",
    "In the paper by Lessmann et al., experiments were performed with all scans, but also with only scans reconstructed with the softer (smoother images) or with the sharper (higher contrast, but more noise) reconstruction kernels. The following table gives an overview of the kernels present in this dataset:\n",
    "\n",
    "| Vendor  | Kernel    | Classification  |\n",
    "| ------- | --------- | --------------- |\n",
    "| GE      | STANDARD  | Soft            |\n",
    "|         | BONE      | Sharp           |\n",
    "|         | LUNG      | Sharp           |\n",
    "| Siemens | B30f      | Soft            |\n",
    "|         | B50f      | Sharp           |\n",
    "|         | B80f      | Sharp           |\n",
    "| Philips | C         | Soft            |\n",
    "|         | D         | Sharp           |\n",
    "| Toshiba | FC10      | Soft            |\n",
    "|         | FC51      | Sharp           |\n",
    "\n",
    "This data is summarized in the file \"dataset.csv\", which we therefore read and parse into a dictionary. An example of this file can be found in the scr folder.\n",
    "\n",
    "\n",
    "In the paper by van Velzen et al., several (large) datasets were used for evaluation of the calcium scoring method. These were in total 7240 subjects that received a coronary artery calcium (CAC) scoring CT, diagnostic chect CT, PET attenuation correction CT, radiotherapy treatment planning CT, CAC-screening CT or low-dose chest CT. For further details on the datasets, please refer to the Radiology paper cited above. In this paper the performance of the method, as developed by Lessmann et al (trained with low-dose chest CT from the NLST), was evaluated in the different types of CTs. Moreover, it was investigated wether the method can adapt to different types of CT when representative images are added to the training dataset of NLST scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = calciumscoring.datasets.read_metadata_file(path.join(input_dir, 'dataset.csv'))\n",
    "print('Loaded metadata of {}/1687 images'.format(len(metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image resampling\n",
    "\n",
    "In clinical practice, calcium scoring scans are dedicated cardiac scans with cardiac gating and a standardized axial resolution of 3mm slice thickness and 3mm slice spacing (i.e. non-overlapping slices). Chest CT scans are usually reconstructed to much thinner slices. In our dataset, the slice thickness ranged from 1mm to 3mm and the slice spacing from 0.6mm to 3mm. We therefore resampled the original images to a slice thickness of 3mm and a slice spacing of 1.5mm, which results in overlapping slices (which has been shown to improve the reproducibility of calcium scoring compared to the default 3mm spacing).\n",
    "\n",
    "Because images become softer with increasing slice thickness due to stronger partial volume effects, we immitated partial voluming by using a weighted average resampling strategy. For each new 3mm slice, the overlap with slices at the original resolution is determined and the values from each original slice are weighted accordingly:\n",
    "\n",
    "<img src=\"static_figures/weighted_average_resampling.png\" alt=\"Illustration of the weighted average resampling strategy\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that uses this strategy to preprocess images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The resampler is defined in the python package \"calciumscoring\"\n",
    "# Create an instance set to 3mm thick slices with 1.5mm spacing between slices\n",
    "resampler = calciumscoring.resampling.WeightedAverageImageResampler(3.0, 1.5)\n",
    "\n",
    "def preprocess_image(image, spacing, origin, metadata):\n",
    "    # All images should be in feet-to-head orientation, but originally the images were\n",
    "    # resampled before correcting the orientation of images that were originally head-to-feet.\n",
    "    # For compatibility reasons, we therefore flip those images back.\n",
    "    flip_image = metadata['slice_order'] == 'head-to-feet'\n",
    "    \n",
    "    if flip_image:\n",
    "        image = image[::-1, :, :]  # images are represented in order z-y-x !!!\n",
    "    image, spacing, origin = resampler.resample(image, spacing, origin, metadata['slice_thickness'])\n",
    "    if flip_image:\n",
    "        image = image[::-1, :, :]\n",
    "    \n",
    "    # Normalize image values since some vendors use values < -1024 outside the FOV of the scan\n",
    "    image = np.clip(image, -1000, 3096)\n",
    "    \n",
    "    return image, spacing, origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code can be used to load the original DICOM images, preprocess them (i.e. resample to 3mm slice thickness) and save them in the scratch directory.\n",
    "\n",
    "**Warning:** All of the following steps can take a long time, from at least a few hours to 3-4 days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if resampled images are available in the input directory, otherwise\n",
    "# look for the original DICOM files and preprocess (i.e. resample) them\n",
    "input_imagedir = path.join(input_dir, 'images')\n",
    "existing_image_files = glob(path.join(input_imagedir, '*.mha'))\n",
    "\n",
    "if len(existing_image_files) != len(metadata):\n",
    "    # Relocate image directory to scratch folder\n",
    "    input_imagedir = path.join(scratch_dir, 'images')\n",
    "    for image_file in existing_image_files:\n",
    "        shutil.copy(image_file, input_imagedir)\n",
    "    \n",
    "    # Load dicom files, preprocess the image and store in ITK file format\n",
    "    dicom_dir = path.join(input_dir, 'dicom')\n",
    "    n_preprocessed = 0\n",
    "    \n",
    "    for dicom_file in glob(path.join(dicom_dir, '*.dcm')):\n",
    "        series_uid = path.basename(dicom_file)[:-4]\n",
    "        image, spacing, origin = calciumscoring.io.read_dicom_image(dicom_file)\n",
    "        image, spacing, origin = preprocess_image(image, spacing, origin, metadata[series_uid])\n",
    "        calciumscoring.io.write_image(path.join(input_imagedir, series_uid + '.mha'), image, spacing, origin)\n",
    "        n_preprocessed += 1\n",
    "    \n",
    "    print('{} images preprocessed and converted to ITK file format'.format(n_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images now have a standardized slice thickness and slice increment, but can still have different resolution in the x/y-plane. While manual annotations etc were all done on these 3mm images, the images and calcium masks are internally resampled again to the average resolution of 0.66 x 0.66 x 1.5 mm³. For better performance, we can do this once and store the resampled images (this requires around 100 GB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_maskdir = path.join(input_dir, 'annotations')  # contains the reference segmentations (calcium masks)\n",
    "\n",
    "standard_resolution = (1.5, 0.66, 0.66)  # z, y, x\n",
    "standard_imagedir = path.join(scratch_dir, 'images_resampled')\n",
    "standard_maskdir = path.join(scratch_dir, 'annotations_resampled')\n",
    "\n",
    "# Make sure that both image and mask dir exist\n",
    "if not path.exists(standard_imagedir):\n",
    "    os.makedirs(standard_imagedir)\n",
    "if not path.exists(standard_maskdir):\n",
    "    os.makedirs(standard_maskdir)\n",
    "\n",
    "# Iterate over all images and resample them to the standard resolution\n",
    "n_resampled = 0\n",
    "\n",
    "for image_file in glob(path.join(input_imagedir, '*.mha')):\n",
    "    series_uid = path.basename(image_file)[:-4]\n",
    "    standard_image_file = path.join(standard_imagedir, series_uid + '.mha')\n",
    "    standard_mask_file = path.join(standard_maskdir, series_uid + '.mha')\n",
    "    if path.exists(standard_image_file) and path.exists(standard_mask_file):\n",
    "        continue\n",
    "\n",
    "    # Load image, resample to standard resolution, save again\n",
    "    image, spacing, origin = calciumscoring.io.read_image(image_file)\n",
    "    mask = calciumscoring.io.read_image(path.join(input_maskdir, series_uid + '.mha'), only_data=True)\n",
    "    \n",
    "    image = calciumscoring.resampling.resample_image(image, spacing, standard_resolution)\n",
    "    mask = calciumscoring.resampling.resample_mask(mask, spacing, standard_resolution)\n",
    "    \n",
    "    calciumscoring.io.write_image(standard_image_file, image, standard_resolution, origin)\n",
    "    calciumscoring.io.write_image(standard_mask_file, mask, standard_resolution, origin)\n",
    "    \n",
    "    n_resampled += 1\n",
    "\n",
    "print('{} images and masks resampled to standard resolution'.format(n_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual reference standard\n",
    "\n",
    "Calcifications were manually segmented and labeled in all images. Segmentation was done using a region growing tool, that requires a single click per lesion and then uses the standard calcium scoring threshold 130 HU to segment the entire calcified lesion. In the paper by Lessmann et al the observers manually selected individual voxels, where this procedure did not work (due to low image quality, or lesions that extent into multiple vessels, such as both LAD and LCX), the observers manually selected individual voxels. In the paper by van Velzen et al., region growing was used in all scans. Scans in which this was not possible (i.e. due to exessive noise) were excluded. \n",
    "\n",
    "The left main coronary artery (LM) was labeled as LAD since it's often impossible to identify LM in ungated scans. \n",
    "\n",
    "In the segmentation masks, voxels are labeled as follows and translated into labels 0-6 for the networks:\n",
    "\n",
    "| Class        | Label (file) | Label (CNNs) |\n",
    "| ------------ | ------------ | -------------|\n",
    "| Background   | 0            | 0            |\n",
    "| LM and LAD   | 8            | 1            |\n",
    "| LCX          | 9            | 2            |\n",
    "| RCA          | 10           | 3            |\n",
    "| Aorta        | 15           | 4            |\n",
    "| Aortic valve | 53           | 5            |\n",
    "| Mitral valve | 58           | 6            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method overview\n",
    "\n",
    "The method consists of two consecutive CNNs (2.5D, see [A.A.A. Setio et al, TMI 2016](https://dx.doi.org/10.1109/TMI.2016.2536809)). The first network analyzes the entire image and classifies all voxels above the standard calcium threshold (>= 130 HU) as either \"Background\" or as one of the calcium types (see list above). The second network analyzes all detected candidate calcium voxels again and classifies them as either \"no calcium\" (0) or \"calcium\" (1).\n",
    "\n",
    "Steps when training the method:\n",
    "* Resample image (done above)\n",
    "* Train CNN1 with all voxels\n",
    "* Apply CNN1 to all training and validation images\n",
    "* Train CNN2 with the voxels >= 130 HU and classified as calcium by CNN1\n",
    "\n",
    "Because the networks are trained with large numbers of scans, only a few scans are kept in memory at a time. After each epoch (a few minibatches), some scans are removed from the memory and replaced by other scans. This strategy is used for training of both networks.\n",
    "\n",
    "\n",
    "Steps when applying the method to an unseen image:\n",
    "* Resample image (3mm thick slices, standard x/y resolution)\n",
    "* Feed image through CNN1\n",
    "* Feed parts of the image through CNN2\n",
    "* Calculate calcium scores\n",
    "* Determine risk category based on coronary calcium score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1: Candidate detection and anatomical labeling\n",
    "\n",
    "<img src=\"static_figures/CNN1.png\" alt=\"Architecture of the network\" style=\"width: 600px; margin-top: 20px; margin-bottom: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the first stage network (this will take 3-4 days!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/TrainNetwork1.py --inputdir $input_dir --scratchdir $scratch_dir --visdom $visdom_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the first stage network to the data (will also take long, 1-2 days!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/FindCandidatesWithNetwork1.py --inputdir $input_dir --scratchdir $scratch_dir --test_data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/FindCandidatesWithNetwork1.py --inputdir $input_dir --scratchdir $scratch_dir --test_data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/FindCandidatesWithNetwork1.py --inputdir $input_dir --scratchdir $scratch_dir --test_data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2: False-positive reduction\n",
    "\n",
    "<img src=\"static_figures/CNN2.png\" alt=\"Architecture of the network\" style=\"width: 600px; margin-top: 20px; margin-bottom: 10px;\">\n",
    "\n",
    "Figure from Lessmann et al., TMI (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the second stage network (this will again take quite some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/TrainNetwork2.py --inputdir $input_dir --scratchdir $scratch_dir --visdom $visdom_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the second stage network to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/ReclassifyCandidatesWithNetwork2.py --inputdir $input_dir --scratchdir $scratch_dir --test_data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "In the paper by van Velzen et al., regiongrowing with a 130 HU threshold was used to match standard calcium scoring. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/PostprocessRegionGrowing.py --inputdir $input_dir --scratchdir $scratch_dir --test_data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcium scores\n",
    "\n",
    "Calculate Agatston scores (the most commonly used kind of calcium score) and the calcium volume for the different labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/ComputeCalciumScores.py --inputdir $input_dir --scratchdir $scratch_dir --test_data testing --postprocessed False \n",
    "# set postprocessed to True for calculating scores of postprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to calculate the reference scores from the reference annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/ComputeCalciumScores.py --inputdir $input_dir --scratchdir $scratch_dir --score_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the coronary calcium scores to put the subjects into risk categories and compare how well this assignment agrees between manual and reference scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run src/EvaluateRiskCategoryAssignment.py --inputdir $input_dir --scratchdir $scratch_dir --test_data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different reconstruction kernels\n",
    "\n",
    "In the paper by Lessmann et al., different reconstruction kernels (soft tissue / medium-sharp kernels) were separated to investigate whether the method works better on images with soft reconstructions.\n",
    "\n",
    "All of the external scripts executed above accept an additional parameter \"--kernels soft\" or \"--kernels sharp\" to limit the analysis to images reconstructed with either soft or sharp kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
